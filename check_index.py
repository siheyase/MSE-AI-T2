import json
from pathlib import Path
from langchain_ollama import OllamaEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.retrievers.multi_vector import MultiVectorRetriever
from langchain.storage import LocalFileStore

vs_path = Path.cwd() / "vs"
vs_docs_path = Path.cwd() / "docs"
embedding_model = OllamaEmbeddings(model="bge-m3")
vectorstore = FAISS.load_local(vs_path, embedding_model, allow_dangerous_deserialization=True)
docstore = LocalFileStore(vs_docs_path)
retriever = MultiVectorRetriever(
    vectorstore=vectorstore,
    docstore=docstore,
    id_key="doc_id",
    search_type="similarity",   # similarity or mmræœ€å¤§è¾¹é™…ç›¸å…³æ£€ç´¢
    search_kwargs={"k": 3},     # æ§åˆ¶è¿”å›æ–‡æ¡£æ•°é‡
)

# è¿›è¡ŒæŸ¥è¯¢æµ‹è¯•
query = "å‡ºç°å‘¼å¸å›°éš¾æ€ä¹ˆåŠï¼Ÿ"
results = retriever.invoke(query)

for i, raw_bytes in enumerate(results):
    print(f"\n--- åŸå§‹æ–‡æ¡£ {i+1} ---")
    decoded = raw_bytes.decode("utf-8")
    # è½¬æ¢ä¸º JSON å¯¹è±¡ï¼ˆå­—å…¸ï¼‰
    try:
        doc_dict = eval(decoded)
        print(json.dumps(doc_dict, indent=2, ensure_ascii=False))
    except Exception as e:
        print("âš ï¸ è§£ç å¤±è´¥:", e)


# éå† vectorstore ä¸­çš„æ‰€æœ‰åˆ†å—ï¼Œ
# print(f"ğŸ“¦ å½“å‰å‘é‡æ•°é‡: {vectorstore.index.ntotal}")
# print(f"ğŸ”— ç´¢å¼•åˆ°æ–‡æ¡£IDæ˜ å°„æ•°: {len(vectorstore.index_to_docstore_id)}")

# for i in range(vectorstore.index.ntotal):
#     doc_id = vectorstore.index_to_docstore_id[i]
#     doc = vectorstore.docstore.search(doc_id)
#
#     print(f"\n--- Chunk {i+1} ---")
#     print(f"ğŸ†” æ–‡æ¡£ID: {doc_id}")
#     print(f"ğŸ“„ å†…å®¹ç‰‡æ®µ:\n{doc.page_content[:300]}...")
#     print(f"ğŸ“ å…ƒæ•°æ®: {doc.metadata}")


